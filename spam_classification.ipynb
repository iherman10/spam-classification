{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2870c7c-670d-459a-ac4d-10618934b57f",
   "metadata": {},
   "source": [
    "# Frame the problem and look at the big picture\n",
    "This is exercise #4 from Chapter 3 of Hands-On Machine Learning, which covers classification. \n",
    "\n",
    "1. __Define the objective.__ The task is to build a spam classifier using data from [Apache SpamAssassin's public datasets](https://spamassassin.apache.org/old/publiccorpus/) ([README](https://spamassassin.apache.org/old/publiccorpus/readme.html)). \n",
    "\n",
    "2. __How will your solution be used?__ This could be used by an email service like Gmail to automatically flag and filter out messages that have a high probability of being spam. \n",
    "\n",
    "3. __What are the current solutions/workarounds (if any)?__ Well, spam filters do already exist in most if not all email services. That being said, some spam emails still get through, so it's up to users to identify those on their own. In addition, when a spam email does get through, users can mark it as spam, and all future emails from that address will be automatically marked as spam. \n",
    "\n",
    "4. __How should you frame this problem (supervised/unsupervised, online/offline, etc.)?__ This is a __supervised classification task__ because we are making binary predictions (spam vs. ham) and we have labeled training data. Ideally, this is an online learning task, because spammers never stop thinking of new ways to trick people, so the model should be learning from new data constantly. However, for the sake of this exercise, it will be an offline system using static training data. \n",
    "\n",
    "5. __How should performance be measured?__ This is a bit more complex than the Titanic task, for example, where _accuracy_ was our primary performance metric. Is it better to lean further towards spam and risk misclassifying safe emails as spam? Or is it better to lean further towards \"ham\" and risk missing spam emails?\n",
    "    - According to ChatGPT: \"For a spam classifier, it is generally more fitting to __prioritize precision over recall__.\" This is because a spam classifier that marks safe emails as spam is a real nuisance to users, whereas missing a few spam emails is not as big a deal. By focusing on precision, we will minimize false positives and reduce the chances of misclassifying safe emails. \n",
    "    \n",
    "    - `Precision = TP / (TP + FP)`\n",
    "    \n",
    "    - That being said, __there should also be a minimum recall.__ I could predict just one instance as spam, and if I'm correct, that means my precision is 100%. However, recall would be way too low.  \n",
    "    \n",
    "6. __Is the performance measure aligned with the business objective?__ Yes. Optimizing for precision will reduce the number of false positives, and therefore create a better hypothetical user experience. Since the spam classifier would most likely be used in a commercial product, user experience is the most important thing. \n",
    "\n",
    "7. __What would be the minimum performance needed to reach the business objective?__ It seems that a __precision of 95%__ is the widely recommended benchmark for spam classifiers. Regarding minimum recall, maybe let's see baseline performance of different models to estimate what a realistic minimum might be. \n",
    "\n",
    "8. __What are comparable problems? Can you reuse experience or tools?__ Beyond the Titanic dataset, this is really my first end-to-end classification project. From my work on the [MNIST dataset](https://github.com/iherman10/mnist-classification/blob/main/chapter_3.ipynb), I can reuse some charting function to analyze performance and compare models. From my work on the [Titanic dataset](https://github.com/iherman10/titanic/blob/main/titanic_2.ipynb), I can reuse the custom transformer class structures for data transformation purposes.\n",
    "\n",
    "9. __Is human expertise available?__ The internet :) I'm working on this solo. \n",
    "\n",
    "10. __How would you solve the problem manually?__ When I try to eyeball whether an email is spam or not, I consider:\n",
    "    - Have I received emails from this sender before?\n",
    "    - Are there obvious typos?\n",
    "    - Are there links? \n",
    "    - Are they asking for money? Or credit card information? Etc. \n",
    "    - Are they writing in all caps? Or all lower-case? \n",
    "    \n",
    "    These considerations might influence the feature engineering part of this project. \n",
    "\n",
    "11. __List assumptions you've made so far.__ \n",
    "    - I should be able to identify most spam just by looking at it. \n",
    "    \n",
    "    - I'll have to leverage text transformations to process the data. \n",
    "    \n",
    "    - Baseline models should get me most of the way towards my performance goal, and feature engineering/hyperparameter tuning will get me the rest of the way.  \n",
    "\n",
    "12. __Verify assumptions if possible.__ TBD..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cab1e55-f245-4a14-80f0-fc71f9d06f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Libraries \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import os\n",
    "import tarfile \n",
    "import urllib.request\n",
    "\n",
    "import joblib\n",
    "\n",
    "import email \n",
    "import email.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c9d787-66d7-487b-827a-51873a74130c",
   "metadata": {},
   "source": [
    "# Get the data\n",
    "Available data from SpamAssassin website: \n",
    "- `spam`: 500 spam messages \n",
    "- `spam_2`: 1396 spam messages, added more recently. \n",
    "- `easy_ham`: 2500 non-spam messages, fairly easy to differentiate.  \n",
    "- `easy_ham_2`: 144 non-spam messages, added more recently. \n",
    "- `hard_ham`: 250 non-spam messages, harder to differentiate. \n",
    "\n",
    "In total, there are __6046__ messages with about a 31% spam ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5506ba1-2df8-4fce-922e-16449ec8cb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download data \n",
    "DOWNLOAD_ROOT = 'http://spamassassin.apache.org/old/publiccorpus/'\n",
    "\n",
    "SPAM_URL = DOWNLOAD_ROOT + '20030228_spam.tar.bz2'\n",
    "SPAM_2_URL = DOWNLOAD_ROOT + '20050311_spam_2.tar.bz2'\n",
    "EASY_HAM_URL = DOWNLOAD_ROOT + '20030228_easy_ham.tar.bz2'\n",
    "EASY_HAM_2_URL = DOWNLOAD_ROOT + '20030228_easy_ham_2.tar.bz2'\n",
    "HARD_HAM_URL = DOWNLOAD_ROOT + '20030228_hard_ham.tar.bz2'\n",
    "\n",
    "SPAM_PATH = os.path.join('datasets', 'spam')\n",
    "\n",
    "def fetch_spam_data(spam_url=SPAM_URL, \n",
    "                    spam_2_url=SPAM_2_URL, \n",
    "                    easy_ham_url=EASY_HAM_URL, \n",
    "                    easy_ham_2_url=EASY_HAM_2_URL, \n",
    "                    hard_ham_url=HARD_HAM_URL,  \n",
    "                    spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in (('spam.tar.bz2', SPAM_URL), \n",
    "                          ('spam_2.tar.bz2', SPAM_2_URL), \n",
    "                          ('easy_ham.tar.bz2', EASY_HAM_URL), \n",
    "                          ('easy_ham_2.tar.bz2', EASY_HAM_2_URL), \n",
    "                          ('hard_ham.tar.bz2', HARD_HAM_URL)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=spam_path)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a14d30-4e71-47e3-9798-8ca2b9a1647a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816a460b-ded1-493f-a09f-ca05f32f5c84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load all the emails \n",
    "SPAM_DIR = os.path.join(SPAM_PATH, 'spam')\n",
    "SPAM_2_DIR = os.path.join(SPAM_PATH, 'spam_2')\n",
    "EASY_HAM_DIR = os.path.join(SPAM_PATH, 'easy_ham')\n",
    "EASY_HAM_2_DIR = os.path.join(SPAM_PATH, 'easy_ham_2')\n",
    "HARD_HAM_DIR = os.path.join(SPAM_PATH, 'hard_ham')\n",
    "\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]\n",
    "spam_2_filenames = [name for name in sorted(os.listdir(SPAM_2_DIR)) if len(name) > 20]\n",
    "easy_ham_filenames = [name for name in sorted(os.listdir(EASY_HAM_DIR)) if len(name) > 20]\n",
    "easy_ham_2_filenames = [name for name in sorted(os.listdir(EASY_HAM_2_DIR)) if len(name) > 20]\n",
    "hard_ham_filenames = [name for name in sorted(os.listdir(HARD_HAM_DIR)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "057a321c-00e9-41cb-a061-ac6d0b46b3b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spam: 500 files\n",
      "spam_2: 1396 files\n",
      "easy_ham: 2500 files\n",
      "easy_ham_2: 1400 files \n",
      "hard_ham: 250 files \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "spam: {len(spam_filenames)} files\n",
    "spam_2: {len(spam_2_filenames)} files\n",
    "easy_ham: {len(easy_ham_filenames)} files\n",
    "easy_ham_2: {len(easy_ham_2_filenames)} files \n",
    "hard_ham: {len(hard_ham_filenames)} files \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fa3c579-3c3c-4f6f-af39-d313fa7bfa83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse emails \n",
    "def load_email(directory, filename, spam_path=SPAM_PATH):\n",
    "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "    \n",
    "spam_emails = [load_email(directory='spam', filename=name) for name in spam_filenames]\n",
    "spam_2_emails = [load_email(directory='spam_2', filename=name) for name in spam_2_filenames]\n",
    "easy_ham_emails = [load_email(directory='easy_ham', filename=name) for name in easy_ham_filenames]\n",
    "easy_ham_2_emails = [load_email(directory='easy_ham_2', filename=name) for name in easy_ham_2_filenames]\n",
    "hard_ham_emails = [load_email(directory='hard_ham', filename=name) for name in hard_ham_filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3bcec-eb88-459b-bf91-4016660f753e",
   "metadata": {},
   "source": [
    "# Explore the data\n",
    "Let's start by examining examples of ham vs. spam to understand what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae695bd2-c803-4352-8b5d-dc4d85bc429b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date:        Wed, 21 Aug 2002 10:54:46 -0500\n",
      "    From:        Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
      "    Message-ID:  <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "\n",
      "  | I can't reproduce this error.\n",
      "\n",
      "For me it is very repeatable... (like every time, without fail).\n",
      "\n",
      "This is the debug log of the pick happening ...\n",
      "\n",
      "18:19:03 Pick_It {exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace} {4852-4852 -sequence mercury}\n",
      "18:19:03 exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace 4852-4852 -sequence mercury\n",
      "18:19:04 Ftoc_PickMsgs {{1 hit}}\n",
      "18:19:04 Marking 1 hits\n",
      "18:19:04 tkerror: syntax error in expression \"int ...\n",
      "\n",
      "Note, if I run the pick command by hand ...\n",
      "\n",
      "delta$ pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace  4852-4852 -sequence mercury\n",
      "1 hit\n",
      "\n",
      "That's where the \"1 hit\" comes from (obviously).  The version of nmh I'm\n",
      "using is ...\n",
      "\n",
      "delta$ pick -version\n",
      "pick -- nmh-1.0.4 [compiled on fuchsia.cs.mu.OZ.AU at Sun Mar 17 14:55:56 ICT 2002]\n",
      "\n",
      "And the relevant part of my .mh_profile ...\n",
      "\n",
      "delta$ mhparam pick\n",
      "-seq sel -list\n",
      "\n",
      "\n",
      "Since the pick command works, the sequence (actually, both of them, the\n",
      "one that's explicit on the command line, from the search popup, and the\n",
      "one that comes from .mh_profile) do get created.\n",
      "\n",
      "kre\n",
      "\n",
      "ps: this is still using the version of the code form a day ago, I haven't\n",
      "been able to reach the cvs repository today (local routing issue I think).\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________\n",
      "Exmh-workers mailing list\n",
      "Exmh-workers@redhat.com\n",
      "https://listman.redhat.com/mailman/listinfo/exmh-workers\n"
     ]
    }
   ],
   "source": [
    "print(easy_ham_emails[0].get_content().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119895a-8ddc-4b1a-998a-8fe99bbc15ae",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d790d-98ab-4f50-9b1c-32b3f399415d",
   "metadata": {},
   "source": [
    "# Shortlist promising models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21dec3-0ec5-45e1-b92f-b80e9ba5ce5b",
   "metadata": {},
   "source": [
    "# Fine-tune the system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb663123-9e29-42db-ab73-3eb1a8ebe3eb",
   "metadata": {},
   "source": [
    "# Present your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fcbfb-5550-4c28-9741-d6af9cc1f25b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
