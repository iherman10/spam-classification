{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2870c7c-670d-459a-ac4d-10618934b57f",
   "metadata": {},
   "source": [
    "# Frame the problem and look at the big picture\n",
    "This is exercise #4 from Chapter 3 of Hands-On Machine Learning, which covers classification. \n",
    "\n",
    "1. __Define the objective.__ The task is to build a spam classifier using data from [Apache SpamAssassin's public datasets](https://spamassassin.apache.org/old/publiccorpus/) ([README](https://spamassassin.apache.org/old/publiccorpus/readme.html)). \n",
    "\n",
    "2. __How will your solution be used?__ This could be used by an email service like Gmail to automatically flag and filter out messages that have a high probability of being spam. \n",
    "\n",
    "3. __What are the current solutions/workarounds (if any)?__ Well, spam filters do already exist in most if not all email services. That being said, some spam emails still get through, so it's up to users to identify those on their own. In addition, when a spam email does get through, users can mark it as spam, and all future emails from that address will be automatically marked as spam. \n",
    "\n",
    "4. __How should you frame this problem (supervised/unsupervised, online/offline, etc.)?__ This is a __supervised classification task__ because we are making binary predictions (spam vs. ham) and we have labeled training data. Ideally, this is an online learning task, because spammers never stop thinking of new ways to trick people, so the model should be learning from new data constantly. However, for the sake of this exercise, it will be an offline system using static training data. \n",
    "\n",
    "5. __How should performance be measured?__ This is a bit more complex than the Titanic task, for example, where _accuracy_ was our primary performance metric. Is it better to lean further towards spam and risk misclassifying safe emails as spam? Or is it better to lean further towards \"ham\" and risk missing spam emails?\n",
    "    - According to ChatGPT: \"For a spam classifier, it is generally more fitting to __prioritize precision over recall__.\" This is because a spam classifier that marks safe emails as spam is a real nuisance to users, whereas missing a few spam emails is not as big a deal. By focusing on precision, we will minimize false positives and reduce the chances of misclassifying safe emails. \n",
    "    \n",
    "    - `Precision = TP / (TP + FP)`\n",
    "    \n",
    "    - That being said, __there should also be a minimum recall.__ I could predict just one instance as spam, and if I'm correct, that means my precision is 100%. However, recall would be way too low.  \n",
    "    \n",
    "6. __Is the performance measure aligned with the business objective?__ Yes. Optimizing for precision will reduce the number of false positives, and therefore create a better hypothetical user experience. Since the spam classifier would most likely be used in a commercial product, user experience is the most important thing. \n",
    "\n",
    "7. __What would be the minimum performance needed to reach the business objective?__ It seems that a __precision of 95%__ is the widely recommended benchmark for spam classifiers. Regarding minimum recall, maybe let's see baseline performance of different models to estimate what a realistic minimum might be. \n",
    "\n",
    "8. __What are comparable problems? Can you reuse experience or tools?__ Beyond the Titanic dataset, this is really my first end-to-end classification project. From my work on the [MNIST dataset](https://github.com/iherman10/mnist-classification/blob/main/chapter_3.ipynb), I can reuse some charting function to analyze performance and compare models. From my work on the [Titanic dataset](https://github.com/iherman10/titanic/blob/main/titanic_2.ipynb), I can reuse the custom transformer class structures for data transformation purposes.\n",
    "\n",
    "9. __Is human expertise available?__ The internet :) I'm working on this solo. \n",
    "\n",
    "10. __How would you solve the problem manually?__ When I try to eyeball whether an email is spam or not, I consider:\n",
    "    - Have I received emails from this sender before?\n",
    "    - Are there obvious typos?\n",
    "    - Are there links? \n",
    "    - Are they asking for money? Or credit card information? Etc. \n",
    "    - Are they writing in all caps? Or all lower-case? \n",
    "    \n",
    "    These considerations might influence the feature engineering part of this project. \n",
    "\n",
    "11. __List assumptions you've made so far.__ \n",
    "    - I should be able to identify most spam just by looking at it. \n",
    "    \n",
    "    - I'll have to leverage text transformations to process the data. \n",
    "    \n",
    "    - Baseline models should get me most of the way towards my performance goal, and feature engineering/hyperparameter tuning will get me the rest of the way.  \n",
    "\n",
    "12. __Verify assumptions if possible.__ TBD..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4cab1e55-f245-4a14-80f0-fc71f9d06f48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifacts-prod-use1.pinadmin.com/artifactory/api/pypi/pinterest-python-pip-prod-virtual/simple/\n",
      "Requirement already satisfied: urlextract in /Users/iherman/anaconda3/lib/python3.11/site-packages (1.8.0)\n",
      "Requirement already satisfied: idna in /Users/iherman/anaconda3/lib/python3.11/site-packages (from urlextract) (3.4)\n",
      "Requirement already satisfied: uritools in /Users/iherman/anaconda3/lib/python3.11/site-packages (from urlextract) (4.0.2)\n",
      "Requirement already satisfied: platformdirs in /Users/iherman/anaconda3/lib/python3.11/site-packages (from urlextract) (2.5.2)\n",
      "Requirement already satisfied: filelock in /Users/iherman/anaconda3/lib/python3.11/site-packages (from urlextract) (3.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Libraries \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import os\n",
    "import tarfile \n",
    "import urllib.request\n",
    "\n",
    "import joblib\n",
    "\n",
    "import email \n",
    "import email.policy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from html import unescape\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import html\n",
    "import nltk\n",
    "\n",
    "# Use % to install in currently running environment, not ! \n",
    "%pip install urlextract\n",
    "import urlextract\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c9d787-66d7-487b-827a-51873a74130c",
   "metadata": {},
   "source": [
    "# Get the data\n",
    "Available data from SpamAssassin website: \n",
    "- `spam`: 500 spam messages \n",
    "- `spam_2`: 1396 spam messages, added more recently. \n",
    "- `easy_ham`: 2500 non-spam messages, fairly easy to differentiate.  \n",
    "- `easy_ham_2`: 144 non-spam messages, added more recently. \n",
    "- `hard_ham`: 250 non-spam messages, harder to differentiate. \n",
    "\n",
    "In total, there are __6046__ messages with about a 31% spam ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5506ba1-2df8-4fce-922e-16449ec8cb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download data \n",
    "DOWNLOAD_ROOT = 'http://spamassassin.apache.org/old/publiccorpus/'\n",
    "\n",
    "SPAM_URL = DOWNLOAD_ROOT + '20030228_spam.tar.bz2'\n",
    "SPAM_2_URL = DOWNLOAD_ROOT + '20050311_spam_2.tar.bz2'\n",
    "EASY_HAM_URL = DOWNLOAD_ROOT + '20030228_easy_ham.tar.bz2'\n",
    "EASY_HAM_2_URL = DOWNLOAD_ROOT + '20030228_easy_ham_2.tar.bz2'\n",
    "HARD_HAM_URL = DOWNLOAD_ROOT + '20030228_hard_ham.tar.bz2'\n",
    "\n",
    "SPAM_PATH = os.path.join('datasets', 'spam')\n",
    "\n",
    "def fetch_spam_data(spam_url=SPAM_URL, \n",
    "                    spam_2_url=SPAM_2_URL, \n",
    "                    easy_ham_url=EASY_HAM_URL, \n",
    "                    easy_ham_2_url=EASY_HAM_2_URL, \n",
    "                    hard_ham_url=HARD_HAM_URL,  \n",
    "                    spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in (('spam.tar.bz2', SPAM_URL), \n",
    "                          ('spam_2.tar.bz2', SPAM_2_URL), \n",
    "                          ('easy_ham.tar.bz2', EASY_HAM_URL), \n",
    "                          ('easy_ham_2.tar.bz2', EASY_HAM_2_URL), \n",
    "                          ('hard_ham.tar.bz2', HARD_HAM_URL)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=spam_path)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81a14d30-4e71-47e3-9798-8ca2b9a1647a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "816a460b-ded1-493f-a09f-ca05f32f5c84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load all the emails \n",
    "SPAM_DIR = os.path.join(SPAM_PATH, 'spam')\n",
    "SPAM_2_DIR = os.path.join(SPAM_PATH, 'spam_2')\n",
    "EASY_HAM_DIR = os.path.join(SPAM_PATH, 'easy_ham')\n",
    "EASY_HAM_2_DIR = os.path.join(SPAM_PATH, 'easy_ham_2')\n",
    "HARD_HAM_DIR = os.path.join(SPAM_PATH, 'hard_ham')\n",
    "\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]\n",
    "spam_2_filenames = [name for name in sorted(os.listdir(SPAM_2_DIR)) if len(name) > 20]\n",
    "easy_ham_filenames = [name for name in sorted(os.listdir(EASY_HAM_DIR)) if len(name) > 20]\n",
    "easy_ham_2_filenames = [name for name in sorted(os.listdir(EASY_HAM_2_DIR)) if len(name) > 20]\n",
    "hard_ham_filenames = [name for name in sorted(os.listdir(HARD_HAM_DIR)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "057a321c-00e9-41cb-a061-ac6d0b46b3b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spam: 500 files\n",
      "spam_2: 1396 files\n",
      "easy_ham: 2500 files\n",
      "easy_ham_2: 1400 files \n",
      "hard_ham: 250 files \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "spam: {len(spam_filenames)} files\n",
    "spam_2: {len(spam_2_filenames)} files\n",
    "easy_ham: {len(easy_ham_filenames)} files\n",
    "easy_ham_2: {len(easy_ham_2_filenames)} files \n",
    "hard_ham: {len(hard_ham_filenames)} files \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fa3c579-3c3c-4f6f-af39-d313fa7bfa83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse emails \n",
    "def load_email(directory, filename, spam_path=SPAM_PATH):\n",
    "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "    \n",
    "spam_emails = [load_email(directory='spam', filename=name) for name in spam_filenames]\n",
    "spam_2_emails = [load_email(directory='spam_2', filename=name) for name in spam_2_filenames]\n",
    "easy_ham_emails = [load_email(directory='easy_ham', filename=name) for name in easy_ham_filenames]\n",
    "easy_ham_2_emails = [load_email(directory='easy_ham_2', filename=name) for name in easy_ham_2_filenames]\n",
    "hard_ham_emails = [load_email(directory='hard_ham', filename=name) for name in hard_ham_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b50bf387-21b5-470f-8d8f-f2695a92e232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create train and test sets \n",
    "X = np.array(spam_emails \\\n",
    "             + spam_2_emails \\\n",
    "             + easy_ham_emails \\\n",
    "             + easy_ham_2_emails \\\n",
    "             + hard_ham_emails \\\n",
    "             , dtype=object)\n",
    "\n",
    "y = np.array([1] * len(spam_emails) \\\n",
    "             + [1] * len(spam_2_emails) \\\n",
    "             + [0] * len(easy_ham_emails) \\\n",
    "             + [0] * len(easy_ham_2_emails) \\\n",
    "             + [0] * len(hard_ham_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3bcec-eb88-459b-bf91-4016660f753e",
   "metadata": {},
   "source": [
    "# Explore the data\n",
    "Let's start by examining examples of ham vs. spam to understand what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae695bd2-c803-4352-8b5d-dc4d85bc429b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin A posted:\n",
      "Tassos Papadopoulos, the Greek sculptor behind the plan, judged that the\n",
      " limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\n",
      " Mount Athos monastic community, was ideal for the patriotic sculpture. \n",
      " \n",
      " As well as Alexander's granite features, 240 ft high and 170 ft wide, a\n",
      " museum, a restored amphitheatre and car park for admiring crowds are\n",
      "planned\n",
      "---------------------\n",
      "So is this mountain limestone or granite?\n",
      "If it's limestone, it'll weather pretty fast.\n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "4 DVDs Free +s&p Join Now\n",
      "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n"
     ]
    }
   ],
   "source": [
    "# Ham\n",
    "print(easy_ham_emails[1].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "517389dd-5b8d-44b8-aba3-c44a2c5937ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help wanted.  We are a 14 year old fortune 500 company, that is\n",
      "growing at a tremendous rate.  We are looking for individuals who\n",
      "want to work from home.\n",
      "\n",
      "This is an opportunity to make an excellent income.  No experience\n",
      "is required.  We will train you.\n",
      "\n",
      "So if you are looking to be employed from home with a career that has\n",
      "vast opportunities, then go:\n",
      "\n",
      "http://www.basetel.com/wealthnow\n",
      "\n",
      "We are looking for energetic and self motivated people.  If that is you\n",
      "than click on the link and fill out the form, and one of our\n",
      "employement specialist will contact you.\n",
      "\n",
      "To be removed from our link simple go to:\n",
      "\n",
      "http://www.basetel.com/remove.html\n",
      "\n",
      "\n",
      "4139vOLW7-758DoDY1425FRhM1-764SMFc8513fCsLl40\n"
     ]
    }
   ],
   "source": [
    "# Spam\n",
    "print(spam_emails[6].get_content().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a8e2a-e9d1-4246-9fa4-ae96f74d4423",
   "metadata": {},
   "source": [
    "Some emails are multipart, with images and attachments. Let's look at different types of structures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e6c2431-e1f6-44c4-bce8-4ef2d4cf7dda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email \n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return f'multipart({\", \".join([get_email_structure(sub_email) for sub_email in payload])})'\n",
    "    else:\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cda15fd5-b441-4039-8e23-bbcb659e2a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ac16c80-ec89-439b-a107-779047b681eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2408),\n",
       " ('multipart(text/plain, application/pgp-signature)', 66),\n",
       " ('multipart(text/plain, text/html)', 8),\n",
       " ('multipart(text/plain, text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, text/enriched)', 1),\n",
       " ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n",
       " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain, video/mng)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain))', 1),\n",
       " ('multipart(text/plain, application/x-pkcs7-signature)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n",
       "  1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n",
       "  1),\n",
       " ('multipart(text/plain, application/x-java-applet)', 1)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(easy_ham_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8519ac60-38ce-41ce-ad78-5d8282410ea8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 218),\n",
       " ('text/html', 183),\n",
       " ('multipart(text/plain, text/html)', 45),\n",
       " ('multipart(text/html)', 20),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 1),\n",
       " ('multipart(text/html, text/plain)', 1),\n",
       " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n",
       " ('multipart/alternative', 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(spam_emails).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1d1ab-5b01-4243-87b3-c7c5448a49c3",
   "metadata": {},
   "source": [
    "It seems that ham emails are more often plan text, while spam has a lot of html. Also, a lot of ham emails are signed using \"pgp\", while no spam is. Email structure might be an important feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f02413eb-2626-4468-9832-b4a667f14d67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path : <12a1mailbot1@web.de>\n",
      "Delivered-To : zzzz@localhost.spamassassin.taint.org\n",
      "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id 136B943C32\tfor <zzzz@localhost>; Thu, 22 Aug 2002 08:17:21 -0400 (EDT)\n",
      "Received : from mail.webnote.net [193.120.211.219]\tby localhost with POP3 (fetchmail-5.9.0)\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 13:17:21 +0100 (IST)\n",
      "Received : from dd_it7 ([210.97.77.167])\tby webnote.net (8.9.3/8.9.3) with ESMTP id NAA04623\tfor <zzzz@spamassassin.taint.org>; Thu, 22 Aug 2002 13:09:41 +0100\n",
      "From : 12a1mailbot1@web.de\n",
      "Received : from r-smtp.korea.com - 203.122.2.197 by dd_it7  with Microsoft SMTPSVC(5.5.1775.675.6);\t Sat, 24 Aug 2002 09:42:10 +0900\n",
      "To : dcek1a1@netsgo.com\n",
      "Subject : Life Insurance - Why Pay More?\n",
      "Date : Wed, 21 Aug 2002 20:31:57 -1600\n",
      "MIME-Version : 1.0\n",
      "Message-ID : <0103c1042001882DD_IT7@dd_it7>\n",
      "Content-Type : text/html; charset=\"iso-8859-1\"\n",
      "Content-Transfer-Encoding : quoted-printable\n"
     ]
    }
   ],
   "source": [
    "# Examine email headers \n",
    "for header, value in spam_emails[0].items():\n",
    "    print(header, ':', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fbfddb04-4d02-4836-a012-d46276f84377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Life Insurance - Why Pay More?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just look at the Subject header\n",
    "spam_emails[0]['Subject']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b5da5-0eb4-4ebf-a474-52ddb662245c",
   "metadata": {},
   "source": [
    "## Email to text transformations\n",
    "The emails are easiest to work with in plain text format. We need some functions to transform emails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a22fc75-78e9-4baf-ad0e-d726611c668f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def html_to_plain_text(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6b447255-d0da-4e29-b46e-680de3ccf5f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "\n",
      "<body>\n",
      "\n",
      "<font size=\"2\" PTSIZE=\"10\">\n",
      "<table height=\"338\" cellSpacing=\"0\" cellPadding=\"0\" width=\"539\" border=\"0\" style=\"border:12px double #006600; border-collapse:collapse; padding-left:4; padding-right:4; padding-top:1; padding-bottom:1\" bordercolor=\"#111111\" bordercolorlight=\"#FFFFFF\" bordercolordark=\"#FFFFFF\">\n",
      "  <tr>\n",
      "    <td width=\"4\" height=\"423\">&nbsp;</td>\n",
      "    <td vAlign=\"top\" align=\"left\" width=\"535\" height=\"423\" style=\"border-style: solid; border-width: 0\">\n",
      "    <table cellSpacing=\"0\" cellPadding=\"4\" width=\"95%\" border=\"0\" bgcolor=\"#FFFFFF\" style=\"border-collapse: collapse\" bordercolor=\"#111111\">\n",
      "      <tr>\n",
      "        <td vAlign=\"top\" align=\"right\" height=\"424\">\n",
      "        <table cellSpacing=\"0\" cellPadding=\"0\" width=\"100%\" border=\"0\">\n",
      "          <tr>\n",
      "            <td>\n",
      "            <div align=\"left\">\n",
      "              <font face=\"Verdana, Arial, Helvetica, sans-serif\" size=\"2\"><b>\n",
      "              <font face=\"Verdana, Arial, Helvetica, sans-serif\" color=\"#000000\" size=\"2\">\n",
      "               ...\n"
     ]
    }
   ],
   "source": [
    "html_spam_emails = [email for email in X_train[y_train==1] if get_email_structure(email) == 'text/html']\n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19c1a30f-96f8-401e-bba8-ae82d0d799b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     \n",
      "              Dear\n",
      "              Homeowner,\n",
      " \n",
      "              *6.25%\n",
      "              30 Yr Fixed Rate Mortgage\n",
      "              Interest\n",
      "              rates are at their lowest point in 40 years! We help you find the\n",
      "              best rate for your situation by matching your needs with hundreds\n",
      "              of lenders! Home Improvement, Refinance, Second\n",
      "              Mortgage, Home Equity Loans, and More! Even with less\n",
      "              than perfect credit!\n",
      "               HYPERLINK Click Here for a Free Quote! HYPERLINK\n",
      "            Lock\n",
      "            In YOUR LOW FIXED RATE TODAY\n",
      "              aNO\n",
      "              COST OUT OF POCKET\n",
      "              aNO\n",
      "              OBLIGATION\n",
      "              aFREE\n",
      "              CONSULTATION\n",
      "              aALL\n",
      "              CREDIT GRADES ACCEPTED\n",
      "               HYPERLINK Rates as low as\n",
      "              6.25% won't stay this low forever CLICK HERE\n",
      "               \n",
      "              * based on mortgage rate as of 5-15-02 as low as 6.25% see lender\n",
      "              for detai ...\n"
     ]
    }
   ],
   "source": [
    "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ca492f8d-2b3f-4797-9e52-a0dec09a439e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to convert entire email to plaintext, regardless of format\n",
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2d423b3d-dfb2-4484-aa32-122ac3cf733f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     \n",
      "              Dear\n",
      "              Homeowner,\n",
      " \n",
      "              *6.25%\n",
      "              30 Yr Fixed Rate Mortgage\n",
      "              Interest\n",
      "              rates are at their lowest point in 40 years! We help you find the\n",
      "              best rate for your situation by matching your needs with hundreds\n",
      "              of lenders! Home Improvement, Refinance, Second\n",
      "              Mortgage, Home Equity Loans, and More! Even with less\n",
      "              than perfect credit!\n",
      "               HYPERLINK Click Here for a Free Quote! HYPERLINK\n",
      "            Lock\n",
      "            In YOUR LOW FIXED RATE TODAY\n",
      "              aNO\n",
      "              COST OUT OF POCKET\n",
      "              aNO\n",
      "              OBLIGATION\n",
      "              aFREE\n",
      "              CONSULTATION\n",
      "              aALL\n",
      "              CREDIT GRADES ACCEPTED\n",
      "               HYPERLINK Rates as low as\n",
      "              6.25% won't stay this low forever CLICK HERE\n",
      "               \n",
      "              * based on mortgage rate as of 5-15-02 as low as 6.25% see lender\n",
      "              for detai ...\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample_html_spam)[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28835a69-bfa2-4b37-a82b-8a7a2f2085dc",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming is the process of reducing inflected (or derived) words to their word stem/base/root. E.g. _cat_ from _cats/catlike/catty_. \n",
    "\n",
    "We can use the Natural Language Toolkit ([NLTK](https://www.nltk.org/)) to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "292a93fd-71b5-4163-a7d7-73696ce90736",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations => comput\n",
      "Computation => comput\n",
      "Computing => comput\n",
      "Computed => comput\n",
      "Compute => comput\n",
      "Compulsive => compuls\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
    "    print(word, '=>', stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802f020c-fef6-49bb-ba77-f3c71aca6f97",
   "metadata": {},
   "source": [
    "## Finding URLs\n",
    "We want to replace URLs with the word \"URL\". We can use the the `urlextract` library to do this intead of regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2434bf00-86d6-4990-a8c6-2dab856c4e11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['github.com', 'https://youtu.be/7Pq-S557XQU?t=3m32s']\n"
     ]
    }
   ],
   "source": [
    "url_extractor = urlextract.URLExtract()\n",
    "print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119895a-8ddc-4b1a-998a-8fe99bbc15ae",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "Let's create custom transformer classes to implement the transformations we tested above. Ultimately, the goal is to convert each email into a sparse vector that indicates the presence or absence of each possible word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "26488976-ffb4-4ff2-b495-09ea04633e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5d8b8f1-e6e9-4d51-96bb-87971f551f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'it': 8, 'number': 6, 'no': 6, 'i': 4, 'new': 4, 'a': 3, 'for': 3, 'don': 3, 't': 3, 'url': 2, 'think': 2, 'about': 2, 'not': 2, 'out': 2, 'next': 2, 'if': 2, 'said': 2, 'stop': 2, 'do': 2, 'even': 2, 'date': 1, 'numbertnumb': 1, 'am': 1, 'get': 1, 'mac': 1, 'ani': 1, 'good': 1, 'reason': 1, 'though': 1, 'll': 1, 'tri': 1, 'to': 1, 'wait': 1, 'one': 1, 'of': 1, 'the': 1, 'ibook': 1, 'rumor': 1, 'spring': 1, 'powerbook': 1, 'month': 1, 'model': 1, 'come': 1, 's': 1, 'realli': 1, 'bad': 1, 'idea': 1, 'geez': 1, 'ha': 1, 'an': 1, 'cooler': 1, 'monitor': 1, 'or': 1}),\n",
       "       Counter({'to': 11, 'the': 9, 'number': 6, 'market': 5, 'asset': 4, 'thi': 4, 'we': 4, 'mail': 4, 'system': 3, 'is': 3, 'in': 3, 'agent': 3, 'do': 3, 'at': 3, 'our': 3, 'jennif': 3, 'url': 3, 'not': 3, 'insur': 2, 'industri': 2, 's': 2, 'year': 2, 'll': 2, 'high': 2, 'america': 2, 'qualifi': 2, 'you': 2, 'for': 2, 'phone': 2, 'call': 2, 'or': 2, 'e': 2, 'assetmarketingsystem': 2, 'com': 2, 'profession': 2, 'fastest': 1, 'grow': 1, 'field': 1, 'organ': 1, 'over': 1, 'past': 1, 'four': 1, 'place': 1, 'billion': 1, 'premium': 1, 'sell': 1, 'qualiti': 1, 'commiss': 1, 'fix': 1, 'annuiti': 1, 'million': 1, 'senior': 1, 'citizen': 1, 'whi': 1, 'have': 1, 'so': 1, 'mani': 1, 'chosen': 1, 'busi': 1, 'with': 1, 'onli': 1, 'fmo': 1, 'that': 1, 'gener': 1, 'lead': 1, 'help': 1, 'set': 1, 'appoint': 1, 'structur': 1, 'product': 1, 'posit': 1, 'increas': 1, 'close': 1, 'ratio': 1, 'and': 1, 'handl': 1, 'all': 1, 'paperwork': 1, 'absolut': 1, 'no': 1, 'cost': 1, 'are': 1, 'also': 1, 'proud': 1, 'report': 1, 'routin': 1, 'earn': 1, 'time': 1, 'averag': 1, 'assum': 1, 'pick': 1, 'up': 1, 'entir': 1, 'tab': 1, 'visit': 1, 'corpor': 1, 'offic': 1, 'sunni': 1, 'san': 1, 'diego': 1, 'one': 1, 'can': 1, 'chang': 1, 'your': 1, 'life': 1, 'guarante': 1, 'readi': 1, 'join': 1, 'best': 1, 'susan': 1, 'mailto': 1, 'pleas': 1, 'fill': 1, 'out': 1, 'form': 1, 'below': 1, 'more': 1, 'inform': 1, 'name': 1, 'citi': 1, 'state': 1, 'want': 1, 'anyon': 1, 'receiv': 1, 'who': 1, 'doe': 1, 'wish': 1, 'commun': 1, 'sent': 1, 'be': 1, 'remov': 1, 'from': 1, 'list': 1, 'repli': 1, 'messag': 1, 'instead': 1, 'go': 1, 'here': 1, 'legal': 1, 'notic': 1}),\n",
       "       Counter({'you': 2, 'hyperlink': 2, 'i': 1, 'will': 1, 'show': 1, 'how': 1, 'can': 1, 'quickli': 1, 'and': 1, 'easili': 1, 'improv': 1, 'your': 1, 'credit': 1, 'to': 1, 'a': 1, 'perfect': 1, 'rate': 1, 'click': 1, 'here': 1, 'now': 1, 'for': 1, 'full': 1, 'free': 1, 'detail': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on a few emails \n",
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "X_few_wordcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6943b4-e6c7-426d-aa62-268c804fbdb4",
   "metadata": {},
   "source": [
    "We need to convert our word counts to vectors. To do this, we'll build another transformer:\n",
    "- `fit()`: build the vocabulary (ordered list of the most common words)\n",
    "- `transform()`: use the vocabulary to convert word counts to vectors (sparse matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8d71629b-f093-41f7-9fb3-a08d77b3ada4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e9f95a20-5b9b-4795-9607-8187b867a413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 24 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transfomer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transfomer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d6db31d-5eab-4456-aaab-9ee023dc894e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 60,   6,   1,   1,   8,   6,   3,   2,   4,   2,   2],\n",
       "       [178,   6,  11,   9,   0,   1,   2,   3,   0,   3,   3],\n",
       "       [ 23,   0,   1,   0,   0,   0,   1,   0,   1,   0,   0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f98c6-aa17-4ade-a40b-a3fe87a7f072",
   "metadata": {},
   "source": [
    "How to interpret this matrix: the 178 in the second row, first column, means that the second email contains 178 words that are not part of the vocabulary. The 6 next to it means that the first word in the vocabulary is present 6 times in this email. The 11 next to it means that the second word is present 11 times, and so on. You can look at the vocabulary to know which words we are talking about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e63bc5c0-58c8-46ed-ace1-222a62c0173e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'number': 1,\n",
       " 'to': 2,\n",
       " 'the': 3,\n",
       " 'it': 4,\n",
       " 'no': 5,\n",
       " 'for': 6,\n",
       " 'url': 7,\n",
       " 'i': 8,\n",
       " 'not': 9,\n",
       " 'do': 10}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transfomer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0107981a-14b8-4414-8ddd-f37f6a6edb2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create full pipeline \n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('email_to_wordcount', EmailToWordCounterTransformer()), \n",
    "    ('wordcount_to_vector', WordCounterToVectorTransformer())\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d790d-98ab-4f50-9b1c-32b3f399415d",
   "metadata": {},
   "source": [
    "# Shortlist promising models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21dec3-0ec5-45e1-b92f-b80e9ba5ce5b",
   "metadata": {},
   "source": [
    "# Fine-tune the system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb663123-9e29-42db-ab73-3eb1a8ebe3eb",
   "metadata": {},
   "source": [
    "# Present your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fcbfb-5550-4c28-9741-d6af9cc1f25b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
